\section{Methodology} \label{methodology}

This study proposes an investigation into how the scientific research on the \textit{economics of migration} has evolved from the late 20th century to the present day. Following what was discussed in previous sections, we can ask a few questions: Has the interest in international migration continued to increase throughout the 21st century If so, which topics received the most attention? Besides, how has the research on internal migration evolved after dominating the research agenda during the 20th century? \textit{Our hypothesis is that topics related to internal migration have lost importance, making way for topics related to international migration to increase their participation}. In other words, we expect to see a greater participation in topics related to international migration in the 21st century, to the detriment of those related to internal migration. 

To break down the composition of topics in the \textit{economics of migration}, the method chosen was topic modelling, an unsupervised machine learning method of text analysis, through which we can estimate a topic model. Topic models are generative Bayesian probabilistic models that allow us to uncover the semantic structure underlying a collection of documents \citep{blei_latent_2003, blei_topic_2009, evans_machine_2016}. While topic modelling is a relatively recent method, it has proven to be a promising analytical tool for several areas in the social sciences. For instance, it has been used to uncover the intellectual structure of documents \citep{griffiths_finding_2004}, assess the content and sentiments in speeches and other types of communication \citep{grimmer_bayesian_2010, hansen_transparency_2018}, and for mapping fields of study \citep{ambrosino_what_2018, pisarevskaya_mapping_2020}. 

Among the many types of topic models, one of the most widely used with discrete text data, and at the same time one of the simplest and most straightforward, is Latent Dirichlet Allocation (LDA)\footnote{For a slightly more technical presentation of LDA, see Appendix \ref{LDA}.}, which was first proposed by \cite{blei_latent_2003}. Following \cite{pisarevskaya_mapping_2020}, we decided to estimate an LDA topic model, which will be based on abstracts of scientific works. Intuitively speaking, our problem is that we have a corpus of documents (abstracts of scientific works), each consisting of a mixture of topics, but we do not know a priori which topics the documents are about, and that is mainly what we want to find out, so we can proceed to analyse the evolution of the topical composition.

\subsection{LDA topic modelling} \label{topic_modelling}

Prior to actually estimating our LDA topic model, there are a few steps that need to be followed. We start with a corpus of abstracts of scientific works, each of which is made up of a bunch of words. However, LDA extracts the underlying semantic structure of documents, and many of our words do not carry any semantic content; hence, we have to pre-process our text data\footnote{This is a common step in natural language processing (NLP). For a detailed explanation, see \cite[p. 9-12]{ponweiser_latent_2012}.}. Pre-processing involves steps such as (i) tokenization, which is the process of breaking down a text into smaller units called tokens, which can be words or other characters; (ii) the removal of stop words, punctuation, and other unused words; (iii) the adjustment of words with different spellings (British and American, for example), synonyms, and acronyms; (iv) and lemmatization, which is a process that helps standardize words that appear in different grammatical and orthographical forms. 

After pre-processing our text data, we are left with the words that have a semantic content only, which are the ones that form our fixed vocabulary. With this fixed vocabulary, we shall represent our collection of scientific documents as a document-term matrix (DTM), which represents every document in terms of frequencies of words in our fixed vocabulary. An additional step after creating the DTM is to trim less frequent words using a threshold, which is an arbitrary value defined by the authors. Given that LDA topic modelling is quite intensive, computationally speaking, removing less frequent words reduces the size of our fixed vocabulary, which makes computations easier and more efficient. Once we have the DTM, we can finally proceed to estimate the LDA topic model, which is done using Gibbs sampling\footnote{For the sake of brevity, we will omit any technical discussion about Gibbs sampling, but its use when estimating an LDA model is a standard practice. Another common alternative method is variational expectation-maximization (VEM) \citep{grun_topicmodels_2011}.}.

All the computations and estimations were performed using the statistical software R\footnote{Our code to implement LDA topic modelling was built following \href{https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/r_text_lda.md}{Wouter van Atteveldt and Kasper Welbers (2020)}.}. Packages \textit{quanteda} and \textit{textstem} were used in the text pre-processing, while the function \textit{LDA} from the package \textit{topicmodels}\footnote{This is an R package that provides basic tools for fitting topic models. For a detailed presentation, see \cite{grun_topicmodels_2011}.} was used to fit our LDA model. Three elements on the model need to be defined by us, namely, the hyperparameters $\eta$ and $\alpha$, which are responsible respectively for the sparsity of the distributions of words over topics and topics over documents\footnote{Lower values of both $\eta$ and $\alpha$ lead LDA to fit models with sparse distributions. Usually, the benchmark for what is considered low is anything below 1. Therefore, if $\eta, \alpha < 1$, then each topic will be composed of a smaller number of very high-probability words, and each document will be composed of a smaller number of very high-probability topics.}, and the number of latent topics $K$, which was chosen with the help of the \textit{ldatuning} package\footnote{The \textit{ldatuning} package computes four metrics--Griffiths2004 \citep{griffiths_finding_2004}, CaoJuan2009 \citep{cao_density-based_2009}, Arun2010 \citep{arun_finding_2010}, and Deveaud2014 \citep{deveaud_accurate_2014}--whose results are the statistically optimal number of topics, whereby the researcher may use all of them to make their decision, or only those they deem appropriate.}. It is worth pointing out that no single value is correct for any of these three parameters, so their definition is thus a research decision. Since LDA is a probabilistic generative process, the outcomes of the fitted LDA topic model are matrices of probability distributions, the \textit {matrix of per-topic word probabilities}, and the \textit{matrix of per-document topic probabilities}. 

The \textit{matrix of per-topic word probabilities} displays the posterior probabilities of words belonging to topics, which will help us understand the content of the topics on the \textit{economics of migration}, and consequently label them\footnote{Remember that LDA is an unsupervised technique, meaning that it learns from the unlabeled data we provide, but it is unable to assess and interpret topics qualitatively \citep{evans_machine_2016}. For a discussion on this, see \cite{chang_reading_2009}.}. In essence, topics are lists of all the words in the fixed vocabulary, but some words are more likely to belong to them. Pisarevskaya and colleagues say that ``the 20-30 most probable words for each topic can be helpful in understanding the content of the topic" \citep[p. 460]{pisarevskaya_mapping_2020}. However, in our case, we realized that the 15 most probable words would already be descriptive enough; moreover, presenting fewer words allows us to focus on the most relevant ones for each topic.

The second outcome is the \textit{matrix of per-document topic probabilities}, which exhibits the posterior probabilities of documents belonging to topics. Each document is associated with all the topics with some probability, but some topics are more likely to be the actual topics that the documents deal with. Therefore, these probabilities show the proportion of each topic in a document. Ultimately, these topic proportions can be used to track the trends in topics over time. By grouping the posterior probabilities by topic and year, and averaging them within each year over all of our abstracts, we computed the yearly mean posterior probabilities for each topic, which are considered to be the topic proportions of documents we can expect to have over the years, allowing us to investigate how each topic has changed over time, which is a strategy that can be used to test for our hypothesis.