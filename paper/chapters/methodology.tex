\section{Methodology} \label{methodology}

This study proposes an investigation on how the scientific research on the \textit{economics of migration} has evolved in terms of its topical composition, with the spotlight on the 21st century. Following the discussion in Section \ref{eco_migration}, our hypothesis is that topics related to internal migration, which were prominent in the 20th century, have lost importance, making way for topics related to international migration to increase their participation in the 21st century. Therefore, we expect to see greater participation in topics related to international migration, to the detriment of those related to internal migration. Ultimately, this change would represent a greater diversification in the field of \textit{economics of migration}, that is, a greater diversity in the field's topical structure.

Intuitively speaking, our problem is that we have a corpus of documents, each consisting of a mixture of topics, but we do not know a priori which topics the documents are about, and that's mainly what we want to find out, so we can proceed to analyse the evolution of the topical composition in our collection of documents. To identify the latent topics in the corpus, we decided to use topic modelling, which is an unsupervised machine learning method of text analysis \citep{evans_machine_2016}. Topic modelling is a strategy through which we can fit a topic model, which in turn are generative Bayesian probabilistic models that allow us to uncover the semantic structure underlying a collection of documents \citep{blei_latent_2003, blei_topic_2009}. Among the many types of topic models, one of the most widely used with discrete text data, and at the same time one of the simplest and most straightforward, is Latent Dirichlet Allocation (LDA), which we briefly present in Appendix \ref{LDA}. Motivated by \cite{pisarevskaya_mapping_2020}, we will fit an LDA topic model on the abstracts of scientific works obtained from the Web of Science (WoS) database.

\subsection{LDA topic modelling} \label{topic_modelling}

In a descriptive way, our entire process of performing LDA topic modelling can be summarized as follows: we started with a corpus of $M$ abstracts of scientific works, each of them made up of a bunch of words. However, LDA extracts the underlying semantic structure of documents, and many of our words do not carry any semantic content; hence, we had to pre-process our text data\footnote{This is a common step in natural language processing (NLP). For a detailed explanation, see \cite[p. 9-12]{ponweiser_latent_2012}.}. Pre-processing involves steps such as (i) tokenization, which is the process of breaking down a text into smaller units called tokens, which can be words or other characters; (ii) the removal of stop words, punctuation, and other unused words; (iii) the adjustment of words with different spellings (British and American, for example), synonyms, and acronyms; (iv) and lemmatization, which is a process that helps standardize words that appear in different grammatical and orthographical forms. After pre-processing our text data, we were left with the words that have a semantic content only, which are the ones that form our fixed vocabulary $\{1, \cdots, V \}$. The final step in our data preparation was representing our corpus of $M$ documents as a so-called document-term matrix (DTM)\footnote{This is only possible under the bag-of-words assumption \citep{ponweiser_latent_2012}.}, which represents every document in terms of frequencies of words in our fixed vocabulary $V$. Also, after creating the DTM, we can trim less frequent words using an arbitrary threshold. Given that LDA topic modelling is quite intensive, computationally speaking, removing less frequent words reduces the size of our fixed vocabulary $V$, which makes computations easier and more efficient.

\begin{table}[]
	\centering
	\begin{tabular}{c|cccccc}
           		        & Word 1 ($w_{1}$) & Word 2 ($w_{2}$) & $\cdots$ & Word $n$ ($w_{n}$) & $\cdots$ & Word $V$ ($w_{V}$) \\ \hline
Document 1 ($\mathbf{w}_{1}$) & 1 & 2 & $\cdots$ & 4 & $\cdots$ & 0 \\
Document 2 ($\mathbf{w}_{2}$) & 0 & 4 & $\cdots$ & 3 & $\cdots$ & 0 \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
Document $m$ ($\mathbf{w}_{m}$) & 2 & 2 & $\cdots$ & 0 & $\cdots$ & 1 \\
$\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ & $\vdots$ \\
Document $M$ ($\mathbf{w}_{M}$) & 5 & 0 & $\cdots$ & 1 & $\cdots$ & 2
	\end{tabular}
	\caption{Hypothetical example of a document-term matrix with $M$ documents and $V$ words, adapted from \cite{ponweiser_latent_2012}. The cells indicate how often a word shows up in a document.}
	\label{tab:dtm}
\end{table}

Once we have the document-term matrix, we can proceed to estimate the LDA topic model using Gibbs sampling\footnote{For the sake of brevity, we will omit any technical discussion of Gibbs sampling, but its use when estimating an LDA model is a standard practice. Another common alternative method is variational expectation-maximization (VEM) \citep{grun_topicmodels_2011}.}. All the computations were done using the statistical software R\footnote{Our code to implement LDA topic modelling was built following \href{https://github.com/ccs-amsterdam/r-course-material/blob/master/tutorials/r_text_lda.md}{Wouter van Atteveldt and Kasper Welbers (2020)}. The data repository with all the replication files is available in: .}. Packages \textit{quanteda} and \textit{textstem} were used in the text pre-processing, while the function \textit{LDA} from the package \textit{topicmodels}\footnote{This is an R package that provides basic tools for fitting topic models. For a detailed presentation, see \cite{grun_topicmodels_2011}.} was used to fit our LDA model. There are three elements on the model that need to be defined by us, namely, the hyperparameters $\eta$ and $\alpha$---which are responsible for the sparsity of the distributions of words over topics and topics over documents\footnote{That is, lower values of both $\eta$ and $\alpha$ lead LDA to fit models with sparse distributions. Usually, the benchmark for what is considered low is anything below 1. Therefore, if $\eta, \alpha < 1$, then each topic will be composed by a smaller number of very high-probability words, and each document will be composed by a smaller number of very high-probability topics.}, respectively---and the number of latent topics $K$, which was chosen with the help of the \textit{ldatuning} package\footnote{The \textit{ldatuning} package computes four metrics---Griffiths2004 \citep{griffiths_finding_2004}, CaoJuan2009 \citep{cao_density-based_2009}, Arun2010 \citep{arun_finding_2010}, and Deveaud2014 \citep{deveaud_accurate_2014}---whose results are the statistically optimal number of topics, whereby the researcher may use all of them to make their decision, or only those they deem appropriate.}. It's worth pointing out that there are no single values that are the correct ones for any of these three parameters, so their definition is thus a research decision. Since LDA is a probabilistic generative process, the fitted LDA topic model produced as outcomes two matrices of probability distributions, the \textit {matrix of per-topic word probabilities} and the \textit{matrix of per-document topic probabilities}. 

The \textit{matrix of per-topic word probabilities} displays the posterior probabilities of words belonging to topics, which will help us understand the content of the topics on the \textit{economics of migration}, and consequently label them\footnote{Remember that LDA is an unsupervised technique, meaning that it learns from the unlabeled data we provide, but it's unable to assess and interpret topics qualitatively \citep{evans_machine_2016}. For a discussion on this, see \cite{chang_reading_2009}.}. In essence, each topic is a list of all the words in the fixed vocabulary, but some words have a much higher probability of belonging to them. Pisarevskaya and colleagues say that ``the 20-30 most probable words for each topic can be helpful in understanding the content of the topic" \citep[p. 460]{pisarevskaya_mapping_2020}, but we concluded that 10-15 is enough, so this is the number of words per topic we will display to assess them.

The second outcome is the \textit{matrix of per-document topic probabilities}, which exhibits the posterior probabilities of documents belonging to topics. Each document is associated with all the topics with some probability, but some topics have a much higher likelihood to be the actual topics that the documents deal with. Therefore, these probabilities show the proportion of each topic in a document (remember that a document can in fact have multiple topics). In the end, these topic proportions were used to perform a \textit{trend topic analysis}, allowing us to follow the trends in topics over time. By grouping the posterior probabilities by topic and year, and averaging them within each year over all of our abstracts, we computed the yearly mean posterior probabilities for each topic, which are considered to be the topic proportions of documents we can expect to have over the years, allowing us to investigate how each topic has changed over time.

To test for our hypothesis that we expect to see a greater diversification in the field of \textit{economics of migration} recently, we used mainly trend topic analysis. As previously stated, the trend topic analysis indicates how the topic proportions changed over time; hence, given our hypothesis, we expect to see a higher proportion of topics related to theories and economic modelling of migration in the late 20th century, but as time goes by we expect to see a decrease in the share of these topics, while other topics increase theirs, showing a greater proportion. In the end, the greater diversity in the field's topical structure that we are hypothesizing would be seen by a convergence in topic proportions. 

\subsection{Diversity indices} \label{diversity_indices}

Additionally, following \cite{pisarevskaya_mapping_2020}, we decided to present two diversity indices to supplement our trend topic analysis with the use of quantitative measures of diversity. Two of the most famous measures of diversity that have been used in many sciences are the Gini-Simpson index and the Shannon-Wiener index \citep{jost_entropy_2006}. The Gini-Simpson index was first proposed by \cite{simpson_measurement_1949}, and it can be used to measure the probability of two entities representing different types or species. The formula of the Gini-Simpson index adapted to our study, here referred to as $GS$, is given by:

\begin{equation}
GS = 1 - \sum^{T}_{i = 1} (p_{i})^{2},
\label{gs_entropy}
\end{equation}

\noindent where $T$ is the number of topics, and $p_{i}$ is the topic proportion, with $GS \in [0,1)$. Applied to our case, the index measures the probability that two abstracts of scientific documents selected at random represent different topics. Therefore, the higher the value of $GS$, the more diverse is our corpus because the higher is the probability that two documents randomly selected are from different topics.

The Shannon-Wiener index, also called the Shannon entropy, is a measure of diversity related to uncertainty. It was first presented by \cite{shannon_mathematical_1948}, and it measures the uncertainty of identifying an entity at random in a certain group or collection. The index, here referred to as $H$, has the following formula:

\begin{equation}
H = - \sum^{T}_{i = 1} p_{i} \ln{p_{i}},
\label{h_entropy}
\end{equation}

\noindent where $T$ is the number of topics, and $p_{i}$ is the topic proportion, with the value of $H \in [0, \infty)$, where the higher $H$ is, the more diverse our corpus, since we have a greater uncertainty about which topic the document is about when choosing a document randomly in a very diverse collection of documents.

It's worth mentioning that there is a discussion proposed by \cite{jost_entropy_2006}, where the author points out that indices like Gini-Simpson and Shannon-Wiener are actually entropies and not diversities, and follows to propose variations that truly measure diversity. However, Jost also admits that ``entropies are reasonable indices of diversity" \citep[p. 363]{jost_entropy_2006}, leaving it clear that the discussion is much more conceptual. Following this discussion, we decided to present only the indices defined above. In any case, acknowledging the differences, we took the decision to call our indices \textit{Gini-Simpson entropy index} and \textit{Shannon-Wiener entropy index}.